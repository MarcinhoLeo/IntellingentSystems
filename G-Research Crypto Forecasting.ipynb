{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled14.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP6SX5QsWO66"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import json\n",
        "import requests\n",
        "import functools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, models, transforms"
      ],
      "metadata": {
        "id": "it5hpxkaWUJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка данных"
      ],
      "metadata": {
        "id": "0djEdyHtWWjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('../input/g-research-crypto-forecasting/train.csv', nrows=10000)\n",
        "df_train.dropna(axis = 0, inplace = True)\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "zcI2OBwvWagd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data = train_test_split(df_train, test_size=0.2, shuffle=False)\n",
        "print(f\"Training data size: {training_data.shape}\",\n",
        "      f\"Validation data size: {validation_data.shape}\")"
      ],
      "metadata": {
        "id": "9-T1uOn_Wdc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Параметры модели"
      ],
      "metadata": {
        "id": "aZftSWN_Wl92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS        = 1000\n",
        "DROPOUT       = 0.15\n",
        "DIRECTIONS    = 1\n",
        "NUM_LAYERS    = 2 \n",
        "BATCH_SIZE    = 5\n",
        "OUTPUT_SIZE   = 1\n",
        "SEQ_LENGTH    = 60 \n",
        "NUM_FEATURES  = 6\n",
        "HIDDEN_SIZE   = 100\n",
        "LEARNING_RATE = 0.00001\n",
        "STATE_DIM     = NUM_LAYERS * DIRECTIONS, BATCH_SIZE, HIDDEN_SIZE\n",
        "TARGET        = \"Target\"\n",
        "FEATURES      = ['Close','High', 'Low', 'Open', 'VWAP', 'Volume']"
      ],
      "metadata": {
        "id": "Yh3Oo9ujWhj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Датасет"
      ],
      "metadata": {
        "id": "Y_0g1EFPWo1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CryptoDataset(Dataset):\n",
        "    def __init__(self, csv_file, seq_length, features, target):\n",
        "        self.csv_file = csv_file\n",
        "        self.target = target\n",
        "        self.features = features\n",
        "        self.seq_length = seq_length\n",
        "        self.data_length = len(csv_file)\n",
        "\n",
        "        self.metrics = self.create_xy_pairs()\n",
        "\n",
        "    def create_xy_pairs(self):\n",
        "        pairs = []\n",
        "        for idx in range(self.data_length - self.seq_length):\n",
        "            x = self.csv_file[idx:idx + self.seq_length][self.features].values\n",
        "            y = self.csv_file[idx + self.seq_length:idx + self.seq_length + 1][self.target].values\n",
        "            pairs.append((x, y))\n",
        "        return pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metrics)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.metrics[idx]"
      ],
      "metadata": {
        "id": "B-tSNXbtWrmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'batch_size': BATCH_SIZE,\n",
        "          'shuffle': False,\n",
        "          'drop_last': True,\n",
        "          'num_workers': 2}\n",
        "params_test = {'batch_size': 1,\n",
        "          'shuffle': False,\n",
        "          'drop_last': False,\n",
        "          'num_workers': 2}\n",
        "\n",
        "training_ds = CryptoDataset(training_data, SEQ_LENGTH, FEATURES, TARGET)\n",
        "training_dl = DataLoader(training_ds, **params)\n",
        "validation_ds = CryptoDataset(validation_data, SEQ_LENGTH, FEATURES, TARGET)\n",
        "validation_dl = DataLoader(validation_ds, **params)"
      ],
      "metadata": {
        "id": "-LRiG3ItWu_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "NRMNi7eYW088"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob, directions=1):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.directions = directions\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "    self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def init_hidden_states(self, batch_size):\n",
        "    state_dim = (self.num_layers * self.directions, batch_size, self.hidden_size)\n",
        "    return (torch.zeros(state_dim).to(device), torch.zeros(state_dim).to(device))\n",
        "\n",
        "  def forward(self, x, states):\n",
        "    x, (h, c) = self.lstm(x, states)\n",
        "    out = self.linear(x)\n",
        "    return out, (h, c)"
      ],
      "metadata": {
        "id": "2fziX5tMW3O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Формируем модель"
      ],
      "metadata": {
        "id": "c55ujy54W7mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM(\n",
        "    NUM_FEATURES,\n",
        "    HIDDEN_SIZE,\n",
        "    NUM_LAYERS,\n",
        "    OUTPUT_SIZE,\n",
        "    DROPOUT\n",
        ").to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.linear.parameters(), lr=LEARNING_RATE, weight_decay=0.01)"
      ],
      "metadata": {
        "id": "4Ci0-r6pW9wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение"
      ],
      "metadata": {
        "id": "2fZ82C1_XAQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(epoch, min_val_loss, model_state, opt_state):\n",
        "  print(f\"New minimum reached at epoch #{epoch + 1}, saving model state...\")\n",
        "  checkpoint = {\n",
        "    'epoch': epoch + 1,\n",
        "    'min_val_loss': min_val_loss,\n",
        "    'model_state': model_state,\n",
        "    'opt_state': opt_state,\n",
        "  }\n",
        "  torch.save(checkpoint, \"./model_state.pt\")\n",
        "def load_checkpoint(path, model, optimizer):\n",
        "    checkpoint = torch.load(path)\n",
        "    min_val_loss = checkpoint[\"min_val_loss\"]\n",
        "    model.load_state_dict(checkpoint[\"model_state\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"opt_state\"])\n",
        "    return model, optimizer, checkpoint[\"epoch\"], min_val_loss\n",
        "\n",
        "\n",
        "def training(model, epochs, validate_every=2):\n",
        "\n",
        "  training_losses = []\n",
        "  validation_losses = []\n",
        "  min_validation_loss = np.Inf\n",
        "\n",
        "  # Set to train mode\n",
        "  model.train()\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "\n",
        "\n",
        "    states = model.init_hidden_states(BATCH_SIZE)\n",
        "    running_training_loss = 0.0\n",
        "\n",
        "    for idx, (x_batch, y_batch) in enumerate(training_dl):\n",
        "      x_batch = x_batch.float().to(device)\n",
        "      y_batch = y_batch.float().to(device)\n",
        "      \n",
        "\n",
        "      states = [state.detach() for state in states]          \n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output, states = model(x_batch, states)\n",
        "\n",
        "      loss = criterion(output[:, -1, :], y_batch)\n",
        "      loss.backward()\n",
        "      running_training_loss += loss.item()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "      optimizer.step()      \n",
        "\n",
        "    training_losses.append(running_training_loss / len(training_dl))\n",
        "        \n",
        "    if epoch % validate_every == 0:\n",
        "      model.eval()\n",
        "      validation_states = model.init_hidden_states(BATCH_SIZE)\n",
        "      running_validation_loss = 0.0\n",
        "      for idx, (x_batch, y_batch) in enumerate(validation_dl): \n",
        "        x_batch = x_batch.float().to(device)\n",
        "        y_batch = y_batch.float().to(device)\n",
        "      \n",
        "        validation_states = [state.detach() for state in validation_states]\n",
        "        output, validation_states = model(x_batch, validation_states)\n",
        "        validation_loss = criterion(output[:, -1, :], y_batch)\n",
        "        running_validation_loss += validation_loss.item()\n",
        "        \n",
        "    validation_losses.append(running_validation_loss / len(validation_dl))\n",
        "  \n",
        "    model.train()\n",
        "\n",
        "    is_best = running_validation_loss / len(validation_dl) < min_validation_loss\n",
        "\n",
        "    if is_best:\n",
        "      min_validation_loss = running_validation_loss / len(validation_dl)\n",
        "      save_checkpoint(epoch + 1, min_validation_loss, model.state_dict(), optimizer.state_dict())\n",
        "        \n",
        "\n",
        "\n",
        "  epoch_count = range(1, len(training_losses) + 1)\n",
        "  plt.plot(epoch_count, training_losses, 'r--')\n",
        "  plt.legend(['Training Loss'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()\n",
        "\n",
        "  val_epoch_count = range(1, len(validation_losses) + 1)\n",
        "  plt.plot(val_epoch_count, validation_losses, 'b--')\n",
        "  plt.legend(['Validation loss'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "3dXZPtkSXFYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training(model, 100)"
      ],
      "metadata": {
        "id": "OqOV_0zvXGSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"./model_state.pt\"\n",
        "model, optimizer, start_epoch, valid_loss_min = load_checkpoint(path, model, optimizer)\n",
        "print(\"model = \", model)\n",
        "print(\"optimizer = \", optimizer)\n",
        "print(\"start_epoch = \", start_epoch)\n",
        "print(\"valid_loss_min = \", valid_loss_min)\n",
        "print(\"valid_loss_min = {:.6f}\".format(valid_loss_min))"
      ],
      "metadata": {
        "id": "PSV7GG-NXVsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JGjNbYXEXYmW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
